from ....logging import inference_logger
import torch

# Pre-calculated expert counts with Mixtral8x7B model on chatbot_arena_conversations dataset.
_NUM_EXPERTS = 8
_TOP_1_EXPERT_COUNTS = [
 [1377133, 1394117, 752510, 715573, 624502, 1074144, 1342941, 675202],
 [1198139, 842613, 1186516, 517134, 951297, 1252068, 1122746, 885609],
 [986132, 1485393, 1005310, 1204850, 950370, 471667, 1032542, 819858],
 [684302, 697793, 648449, 993894, 967176, 1334934, 1206074, 1423500],
 [867367, 1111746, 709091, 1673643, 982594, 1105212, 766739, 739730],
 [867609, 1292767, 566871, 792118, 804549, 1206516, 971353, 1454339],
 [1499869, 656623, 996981, 2588196, 464524, 394788, 849902, 505239],
 [729226, 629103, 730253, 1008638, 638196, 2800194, 598533, 821979],
 [445133, 3577557, 517744, 674253, 442777, 489393, 506830, 1302435],
 [559886, 930226, 3123462, 345206, 348218, 1090547, 434910, 1123667],
 [629865, 400294, 3326949, 636035, 910793, 846906, 759632, 445648],
 [948215, 1305222, 1254132, 272849, 397722, 1768537, 1346528, 662917],
 [419447, 466498, 618102, 4187443, 504491, 392801, 812133, 555207],
 [827667, 3594898, 259415, 1552424, 337462, 447794, 507260, 429202],
 [623487, 471672, 744697, 531641, 976038, 709521, 2951869, 947197],
 [526983, 1183337, 1122645, 874682, 900192, 2004507, 710821, 632955],
 [717458, 1853981, 1994543, 727239, 897214, 498747, 527619, 739321],
 [739646, 667941, 763290, 2051684, 634437, 519140, 1403287, 1176697],
 [1047897, 655436, 866636, 942156, 1027972, 1680814, 652303, 1082908],
 [1005577, 1020635, 849225, 868733, 606075, 1057049, 1830437, 718391],
 [1438688, 961213, 975873, 753148, 770082, 923258, 1293258, 840602],
 [1214051, 828710, 885072, 848711, 1048178, 1201712, 756597, 1173091],
 [1052722, 1378563, 1248278, 925873, 837474, 902594, 733080, 877538],
 [1093787, 1014299, 1104747, 1138924, 919873, 995488, 800390, 888614],
 [1105161, 999102, 1046268, 983433, 732568, 765773, 1280745, 1043072],
 [957800, 910139, 1328490, 736342, 946632, 780024, 1346501, 950194],
 [1211265, 817607, 834076, 1256740, 1099977, 1328579, 434520, 973358],
 [657257, 1628176, 873212, 629958, 1411085, 1001327, 1009754, 745353],
 [871489, 773135, 1438411, 947783, 900912, 1590040, 679597, 754755],
 [1136365, 1185243, 1325356, 1044147, 627070, 667486, 975666, 994789],
 [700385, 706040, 573083, 717580, 709963, 2173395, 572612, 1803064],
 [828080, 1752616, 1993846, 1294785, 575399, 525065, 649159, 337172]]

_TOP_2_EXPERT_COUNTS = [
 [1171472, 645281, 605898, 566251, 805677, 1266409, 1709184, 1185950],
 [1317665, 571201, 1513065, 1403356, 944809, 780297, 671320, 754409],
 [1175340, 1497303, 787639, 1371232, 824638, 659731, 863005, 777234],
 [849321, 828500, 666178, 873371, 1396383, 1340189, 819214, 1182966],
 [1281559, 728059, 888263, 1051590, 1027943, 959449, 852247, 1167012],
 [1097805, 1194935, 736906, 792174, 1374516, 837994, 913907, 1007885],
 [1753574, 669295, 1183143, 1460168, 986417, 396882, 880683, 625960],
 [1006135, 910167, 847456, 625512, 721477, 1763772, 676627, 1404976],
 [510685, 1140334, 911773, 905745, 881805, 418310, 754775, 2432695],
 [540891, 1325096, 1142787, 825743, 787412, 2118372, 448459, 767362],
 [842202, 463720, 1238968, 997394, 1119339, 1531464, 442857, 1320178],
 [982751, 924901, 1551933, 197185, 699419, 1015903, 1669347, 914683],
 [667145, 688729, 589397, 1059266, 910885, 2139359, 1129035, 772306],
 [829849, 1512780, 360298, 2652513, 664567, 702566, 752446, 481103],
 [762563, 572205, 1139359, 494273, 1192811, 529397, 1531943, 1733571],
 [754520, 1513583, 982947, 739363, 875097, 1280972, 819698, 989942],
 [921088, 1112435, 1478909, 936057, 1154101, 666906, 734135, 952491],
 [575885, 776158, 1080264, 1381522, 830803, 674456, 1250236, 1386798],
 [1073869, 1182404, 551336, 966184, 1155639, 1256518, 819633, 950539],
 [1273842, 1047160, 847823, 942834, 951200, 930078, 1222859, 740326],
 [917623, 1150973, 1033499, 1126870, 844983, 775504, 879958, 1226712],
 [1080556, 813472, 828744, 906665, 969086, 1062222, 1409518, 885859],
 [1091089, 1079473, 1265558, 989069, 943039, 844350, 776462, 967082],
 [1050182, 1340656, 848175, 706845, 842749, 1259033, 849531, 1058951],
 [1058722, 1324693, 976682, 1006771, 664824, 683677, 1013133, 1227620],
 [860107, 919058, 1009032, 1502274, 669799, 996327, 1023113, 976412],
 [1064701, 570984, 907070, 1323498, 855688, 1214972, 701249, 1317960],
 [639248, 993465, 844763, 771309, 1252902, 1504410, 1135154, 814871],
 [1102325, 956026, 795116, 1205072, 999233, 1055872, 1098651, 743827],
 [1181077, 1261366, 1058721, 984601, 1008933, 584908, 814934, 1061582],
 [1241365, 497153, 823157, 1435535, 917491, 1279678, 409044, 1352699],
 [761043, 1097615, 722037, 1107737, 797192, 1194823, 615402, 1660273]]

_EXPERT_PROBS = None

def _adjust_num_experts(expert_counts, n_experts):
    if n_experts >= _NUM_EXPERTS:
        return torch.repeat_interleave(expert_counts, n_experts // _NUM_EXPERTS).reshape(-1, n_experts)
    else:
        return expert_counts.reshape(expert_counts.shape[0], n_experts, _NUM_EXPERTS // n_experts).sum(-1)

def init_expert_probs(num_layers, n_experts, n_top_k, temparature, device):
    global _TOP_1_EXPERT_COUNTS
    global _TOP_2_EXPERT_COUNTS
    global _EXPERT_PROBS

    assert num_layers > 0
    assert n_top_k == 1 or n_top_k == 2
    assert _NUM_EXPERTS % n_experts == 0 or n_experts % _NUM_EXPERTS == 0
    if _EXPERT_PROBS is not None:
        assert num_layers == len(_EXPERT_PROBS)
        assert _EXPERT_PROBS[0].shape[0] == n_top_k
        return

    inference_logger().info(f"Initialize simulated gating with temparature={temparature}")
    top1_expert_counts = torch.tensor(_TOP_1_EXPERT_COUNTS, device=device)
    top2_expert_counts = torch.tensor(_TOP_2_EXPERT_COUNTS, device=device)

    top1_expert_counts = _adjust_num_experts(top1_expert_counts, n_experts)
    top2_expert_counts = _adjust_num_experts(top2_expert_counts, n_experts)

    n_experts = top1_expert_counts.shape[1]

    top1_expert_probs = top1_expert_counts / top1_expert_counts.sum(-1, keepdim=True)
    top2_expert_probs = top2_expert_counts / top2_expert_counts.sum(-1, keepdim=True)

    expert_probs = torch.concat((top1_expert_probs, top2_expert_probs), dim=-1).reshape(-1, 2, n_experts).mean(1).reshape(-1, 1, n_experts)
    expert_probs = torch.softmax(torch.log(expert_probs) / temparature, -1)

    if n_top_k == 2:
        expert_probs = expert_probs.repeat(1, 2, 1)

    if num_layers == 1:
        _EXPERT_PROBS = [expert_probs[0]]
    else:
        _EXPERT_PROBS = [
            expert_probs[round(((top1_expert_counts.shape[0] - 1) / (num_layers - 1)) * layer_id)]
            for layer_id in range(num_layers)
        ]

def clear_expert_probs():
    global _EXPERT_PROBS
    _EXPERT_PROBS = None

def get_expert_probs(layer_id):
    global _EXPERT_PROBS
    return _EXPERT_PROBS[layer_id]
